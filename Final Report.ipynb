{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497566a1-5a1e-42b0-b667-020147798270",
   "metadata": {},
   "source": [
    "# **Enhancing Healthcare Insurance Fraud Detection with Machine Learning & AI**\n",
    "#### By: Soham Kaje\n",
    "\n",
    "## **Introduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80064f6e-33ad-4251-b298-3d36fcc7a192",
   "metadata": {},
   "source": [
    "The **Healthcare Insurance Fraud Detection** project explores the critical issue of fraudulent behavior within the U.S. healthcare system, particularly among Medicare providers. Fraudulent claims not only burden insurance companies with excessive costs but also contribute to rising premiums and reduced access to affordable care for patients. Despite existing regulatory efforts, many fraudulent activities remain undetected due to the complexity and scale of healthcare data.\n",
    "\n",
    "This project aims to enhance fraud detection mechanisms by leveraging the power of machine learning and artificial intelligence. By analyzing patterns across inpatient, outpatient, and beneficiary data, we seek to identify behavioral indicators and anomalies that suggest potential fraud. Through this data-driven approach, the project intends to develop a predictive model capable of flagging suspicious providers more accurately and efficiently.\n",
    "\n",
    "Ultimately, our goal is to support insurers, regulators, and healthcare administrators in proactively identifying fraud, thereby reducing costs, improving compliance, and safeguarding the integrity of the healthcare system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a07ec-2f7c-4a21-a657-5f17ba8d12a6",
   "metadata": {},
   "source": [
    "### **Hypotheses**\n",
    "\n",
    "- Providers with higher average reimbursement amounts and more frequent high-cost procedures are more likely to be flagged as fraudulent. This hypothesis explores the assumption that fraud often involves inflating claim costs through unnecessary or misrepresented procedures.\n",
    "- Patient demographics and chronic condition patterns are associated with the likelihood of provider fraud. For example, providers who disproportionately treat older patients with multiple chronic conditions may exploit ambiguities in diagnosis coding, increasing fraud risk.\n",
    "- Providers with a higher number of claims per unique beneficiary are more likely to engage in fraudulent behavior. This aims to test if over-utilization, such as excessive testing or repeated procedures, is a common fraud indicator.\n",
    "\n",
    "By testing these hypotheses, this project aims to uncover systemic patterns that distinguish legitimate care from manipulative billing practices. These insights can refine fraud detection models, reduce false positives, and enhance the accuracy of insurance audits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da577d43-795d-46e3-85f9-1cf898546116",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## **Data Sources**\n",
    "\n",
    "The **Healthcare Insurance Fraud Detection** project leverages four interrelated datasets from a Medicare claims case study hosted on Kaggle. Together, they offer a comprehensive view of medical billing patterns, patient demographics, and provider behaviors essential for uncovering fraudulent activity.\n",
    "\n",
    "### **1. Provider Labels Dataset**\n",
    "This dataset serves as the **ground truth** for supervised machine learning.\n",
    "- **Key Columns**:  \n",
    "  - `Provider`: Unique provider identifier.  \n",
    "  - `PotentialFraud`: Binary label indicating if the provider is suspected of fraud (`Yes`/`No`).\n",
    "\n",
    "This dataset is used to train and evaluate predictive models.\n",
    "\n",
    "### **2. Beneficiary Data**\n",
    "This file includes individual-level **KYC (Know Your Customer)** details for Medicare recipients.\n",
    "- **Demographics**: Gender, race, state, and county.  \n",
    "- **Health Coverage**: Part A/B coverage months, renal disease indicator.  \n",
    "- **Chronic Conditions**: Flags for conditions such as diabetes, stroke, heart failure, and cancer.  \n",
    "- **Financial Data**: Annual inpatient and outpatient reimbursement/deductible amounts.\n",
    "\n",
    "This dataset provides critical context for understanding the types of patients served by providers.\n",
    "\n",
    "### **3. Inpatient Claims Data**\n",
    "Captures claims related to **hospital admissions**.\n",
    "- **Key Variables**:  \n",
    "  - Claim dates (start, end, admission, discharge)  \n",
    "  - Procedure and diagnosis codes (ICD)  \n",
    "  - Reimbursement and deductible amounts  \n",
    "  - Physician identifiers (attending, operating, other)\n",
    "\n",
    "This data helps uncover billing behaviors for more intensive care scenarios.\n",
    "\n",
    "### **4. Outpatient Claims Data**\n",
    "Details claims for **non-admitted** patients, such as checkups, minor treatments, and tests.\n",
    "- **Similar Columns to Inpatient Data**, but generally for lower-cost procedures.\n",
    "- Includes diagnosis and procedure codes, reimbursement, deductible amounts, and claim dates.\n",
    "\n",
    "Together, the inpatient and outpatient datasets are used to calculate provider behavior metrics such as average claim cost, procedure frequency, and patient load.\n",
    "\n",
    "These datasets collectively form the backbone of our fraud detection framework. By integrating and analyzing these sources, the project aims to identify patterns indicative of fraud and build predictive models to flag high-risk providers with greater accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a448bee-8eba-4a16-8a11-6d1a466a9f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Data Cleaning Process**\n",
    "\n",
    "The datasets used in this project were preprocessed and saved in the following folder:\n",
    "\n",
    "- **Cleaned Training Data**: Located in the `Dataset_Cleaned` folder.\n",
    "\n",
    "### **Key Cleaning Steps**\n",
    "\n",
    "#### **1. Claims Data**\n",
    "- Source: `Train-1542865627584.csv`\n",
    "- Selected only the `Provider` and `PotentialFraud` columns to serve as the **target labels** for supervised machine learning.\n",
    "- Removed any unnecessary identifiers and metadata.\n",
    "\n",
    "#### **2. Beneficiary Data**\n",
    "- Source: `Train_Beneficiarydata-1542865627584.csv`\n",
    "- Retained key demographic features (e.g., `Gender`, `Race`, `RenalDiseaseIndicator`) and insurance coverage duration.\n",
    "- Included binary flags for multiple chronic conditions to help capture patient health patterns.\n",
    "- Excluded columns like date of birth and death for simplicity and privacy.\n",
    "- Cleaned version saved as `Cleaned_Train_Beneficiary.csv`.\n",
    "\n",
    "#### **3. Inpatient Claims Data**\n",
    "- Source: `Train_Inpatientdata-1542865627584.csv`\n",
    "- Kept relevant columns such as `ClaimID`, `Provider`, `ClaimStartDt`, `ClaimEndDt`, `InscClaimAmtReimbursed`, and `DeductibleAmtPaid`.\n",
    "- Dropped procedural and diagnostic codes to streamline initial model development.\n",
    "- Cleaned version saved as `Cleaned_Train_Inpatient.csv`.\n",
    "\n",
    "#### **4. Outpatient Claims Data**\n",
    "- Originally split across three files:  \n",
    "  - `Train_Outpatientdata_Part1.csv`  \n",
    "  - `Train_Outpatientdata_Part2.csv`  \n",
    "  - `Train_Outpatientdata_Part3.csv`  \n",
    "  located in the `Dataset` folder.\n",
    "- The files were **merged** into a single DataFrame, and only essential billing and identification fields were retained.\n",
    "- Cleaned version saved as `Cleaned_Train_Outpatient.csv`.\n",
    "\n",
    "### **General Cleaning Actions**\n",
    "- All cleaned datasets were saved in the `Dataset_Cleaned` directory for consistent access.\n",
    "- Irrelevant columns were dropped to reduce noise in model training.\n",
    "- Column names were left in their original format for compatibility with existing notebooks and scripts.\n",
    "- The data cleaning scripts are included in the project repository to ensure transparency and reproducibility.\n",
    "\n",
    "These steps prepared the data for machine learning by focusing on key indicators of fraud while simplifying the feature space for better interpretability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fce48-44fb-4c4a-8bac-f2c978c3bf4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "To understand provider behavior, claim patterns, and patient health profiles, we performed exploratory data analysis (EDA) on the four cleaned datasets. These visualizations helped identify important trends and outliers that guided our feature engineering and model design.\n",
    "\n",
    "### **1. Fraud Label Distribution (Claims Data)**  \n",
    "![Fraud Label Distribution](EDA_Images/fraud_label_distribution.png)\n",
    "\n",
    "**Description**:  \n",
    "This bar chart shows the distribution of providers flagged as potentially fraudulent. As expected, the data is imbalanced, with the majority of providers labeled as non-fraudulent. This class imbalance is a crucial consideration for model selection and evaluation.\n",
    "\n",
    "### **2. Chronic Conditions Among Patients (Beneficiary Data)**  \n",
    "![Chronic Conditions Distribution](EDA_Images/chronic_conditions_distribution.png)\n",
    "\n",
    "**Description**:  \n",
    "The bar chart summarizes the prevalence of various chronic conditions across Medicare beneficiaries. Common issues include ischemic heart disease, diabetes, and heart failure. These health conditions may correlate with the types and frequency of claims submitted by providers.\n",
    "\n",
    "### **3. Inpatient Reimbursement Distribution**  \n",
    "![Inpatient Reimbursement Histogram](EDA_Images/inpatient_reimbursement_distribution.png)\n",
    "\n",
    "**Description**:  \n",
    "This histogram displays the distribution of reimbursement amounts for inpatient claims. While the majority of claims fall within a moderate range, a noticeable number of high-reimbursement outliers could indicate potential overbilling, an important signal in fraud detection.\n",
    "\n",
    "### **4. Outpatient Claim Volume per Provider**  \n",
    "![Outpatient Claims per Provider](EDA_Images/outpatient_claims_per_provider.png)\n",
    "\n",
    "**Description**:  \n",
    "This visualization highlights the top 20 providers by outpatient claim volume. A small number of providers are responsible for a disproportionately high number of claims, which may warrant further investigation. High-volume billing patterns can be red flags for fraudulent behavior.\n",
    "\n",
    "These EDA findings provided a strong foundation for selecting features that capture anomalies, trends, and behavior patterns in healthcare claims. All visualizations are reproducible using the code in `EDA.ipynb` and were generated from the cleaned files located in the `Dataset_Cleaned` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7e434-8839-479b-8e01-c2c89ba6dd75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Visualizations**\n",
    "\n",
    "To support our hypotheses and guide feature selection, we created eight visualizations — two from each dataset — that explore fraud-related patterns in healthcare claims. These visualizations examine trends in claim volume, patient health characteristics, reimbursement behavior, and potential fraud signals.\n",
    "\n",
    "All images are saved in the `Visualization_Images` folder and are generated using `.ipynb` notebooks stored in the `Visualizations` folder.\n",
    "\n",
    "### **1. Claims Data Visualizations**\n",
    "\n",
    "#### **Claim Volume Distribution by Fraud Status**\n",
    "This boxplot (log-scaled) compares simulated claim volumes between providers labeled as fraudulent and non-fraudulent. Fraudulent providers show a broader spread, suggesting overbilling behavior may be common in that group.\n",
    "\n",
    "![Claim Volume vs Fraud](Visualization_Images/claim_volume_vs_fraud_status.png)\n",
    "\n",
    "#### **Fraud Count by Provider Claim Volume Group**\n",
    "Providers were grouped into low, medium, and high claim volume categories. The medium and high-volume groups contain disproportionately more fraud cases, supporting the hypothesis that high-volume billing behavior correlates with fraud risk.\n",
    "\n",
    "![Fraud by Volume Group](Visualization_Images/fraud_by_claim_volume_group.png)\n",
    "\n",
    "### **2. Beneficiary Data Visualizations**\n",
    "\n",
    "#### **Gender Distribution by Fraud Label**\n",
    "This bar chart breaks down patient gender served by fraudulent vs. non-fraudulent providers. While both genders are relatively balanced, fraudulent providers see slightly more female patients — potentially indicating gender-based utilization trends worth further study.\n",
    "\n",
    "![Gender vs Fraud](Visualization_Images/gender_vs_fraud.png)\n",
    "\n",
    "#### **Chronic Condition Correlation Heatmap**\n",
    "This heatmap visualizes the co-occurrence of chronic conditions across beneficiaries. Notably, conditions like diabetes, heart failure, and ischemic heart disease often appear together, which may suggest billing patterns tied to comorbidity clusters.\n",
    "\n",
    "![Chronic Conditions Correlation](Visualization_Images/chronic_conditions_correlation.png)\n",
    "\n",
    "### **3. Inpatient Data Visualizations**\n",
    "\n",
    "#### **Length of Stay vs. Reimbursement (Scatter Plot)**\n",
    "This scatter plot shows that longer hospital stays generally result in higher reimbursements — but fraudulent providers often exhibit higher reimbursements even for shorter stays, suggesting upcoding or inflated charges.\n",
    "\n",
    "![Length of Stay vs Reimbursement](Visualization_Images/length_of_stay_vs_reimbursement.png)\n",
    "\n",
    "#### **Top 10 Providers by Inpatient Reimbursement**\n",
    "This bar chart identifies providers with the highest total inpatient reimbursements. Several providers show outlier behavior, and cross-referencing with fraud labels can help highlight high-risk billing entities.\n",
    "\n",
    "![Top Inpatient Providers](Visualization_Images/top_inpatient_providers.png)\n",
    "\n",
    "### **4. Outpatient Data Visualizations**\n",
    "\n",
    "#### **Monthly Outpatient Claim Volume**\n",
    "This plot shows monthly claim patterns across providers. Slight end-of-year spikes are visible, which may point to seasonal or quota-driven overutilization — a known red flag for potential fraud.\n",
    "\n",
    "![Monthly Outpatient Claims](Visualization_Images/monthly_outpatient_claims.png)\n",
    "\n",
    "#### **Average vs. Median Reimbursement by Fraud Status**\n",
    "This grouped bar chart shows that both the average and median outpatient reimbursement amounts are higher for fraudulent providers. This supports the hypothesis that higher per-claim costs are associated with fraud.\n",
    "\n",
    "![Reimbursement per Claim by Fraud](Visualization_Images/reimbursement_per_claim_by_fraud.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a36429-911d-41d9-8624-98d4c2ac02a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Machine Learning Analysis\n",
    "\n",
    "To uncover patterns that indicate potential healthcare fraud, we developed supervised machine learning models using claims, inpatient, outpatient, and beneficiary data. Below is a summary of our modeling approach, results, and key findings.\n",
    "\n",
    "### 1. Model Objective\n",
    "\n",
    "#### Hypothesis  \n",
    "Certain provider behaviors — such as high claim volume, inflated reimbursements, or patient health patterns — are predictive of fraudulent activity.\n",
    "\n",
    "#### Methodology  \n",
    "- **Input Features**:  \n",
    "  - Claims volume, average reimbursement, deductible amounts  \n",
    "  - Inpatient length of stay, comorbidities (chronic condition ratios)  \n",
    "  - Gender ratio, renal disease rate, and other provider-level aggregates  \n",
    "- **Target Variable**:  \n",
    "  - `PotentialFraud` (binary: **Yes** or **No**)  \n",
    "- **Models Used**:  \n",
    "  - **Random Forest Classifier**  \n",
    "  - **Logistic Regression**  \n",
    "  - **XGBoost Classifier**  \n",
    "- **Data Preprocessing**:  \n",
    "  - Merged and aggregated patient-claim data into provider-level features  \n",
    "  - Handled missing values using median imputation  \n",
    "  - Applied class balancing techniques (e.g., `class_weight='balanced'` and `scale_pos_weight`)\n",
    "\n",
    "### 2. Model Performance\n",
    "\n",
    "#### Summary of Results  \n",
    "\n",
    "| Model               | ROC AUC | Precision (Fraud) | Recall (Fraud) | F1 Score (Fraud) |\n",
    "|--------------------|---------|--------------------|-----------------|------------------|\n",
    "| **Random Forest**   | 0.73    | **0.74** ✅              | 0.49            | 0.59             |\n",
    "| **Logistic Reg.**   | **0.88** ✅ | 0.41               | **0.89** ✅      | 0.56             |\n",
    "| **XGBoost**         | 0.79    | 0.63           | 0.62            | **0.63** ✅       |\n",
    "\n",
    "#### Interpretation  \n",
    "- **Random Forest** was highly precise, but missed many fraudulent providers (low recall).  \n",
    "- **Logistic Regression** identified almost all fraud cases but produced many false positives.  \n",
    "- **XGBoost** delivered the most **balanced results**, making it a strong candidate for production deployment.\n",
    "\n",
    "### 3. Visual Model Comparison\n",
    "\n",
    "We created a side-by-side bar chart comparing model metrics to visualize their trade-offs across ROC AUC, precision, recall, and F1 score.\n",
    "\n",
    "![Model Comparison](ML_Images/model_comparison_plot.png)\n",
    "\n",
    "### 4. Key Findings\n",
    "\n",
    "#### Insights from Model Behavior  \n",
    "- **High Reimbursement + Chronic Comorbidities** → Strong fraud indicators  \n",
    "- **Volume-Heavy Providers** showed elevated fraud risk  \n",
    "- **Balanced classifiers (e.g., XGBoost)** outperformed more rigid ones on minority class detection (fraud = minority)\n",
    "\n",
    "#### Practical Implications  \n",
    "- Machine learning models can effectively support claim audits by **flagging high-risk providers** based on their billing and patient profile data.  \n",
    "- Fine-tuning thresholds or deploying ensembles could further boost real-world accuracy while maintaining fairness.\n",
    "\n",
    "This analysis demonstrates how structured data and predictive modeling can significantly enhance healthcare fraud detection. By combining claims analytics with machine learning, we can reduce investigation costs and improve the integrity of healthcare systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567efd1-e311-4767-96ca-293d839e0f2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Explainable AI with SHAP**\n",
    "\n",
    "In this project, we utilized **SHAP (SHapley Additive exPlanations)**, a state-of-the-art method in **Explainable AI (XAI)**, to understand how our machine learning models make predictions. SHAP offers both **global** and **local** interpretability, ensuring that our models are not just “black-box” classifiers, but transparent systems that can be trusted and audited.\n",
    "\n",
    "### **Why SHAP?**\n",
    "SHAP helps us understand the influence of each feature on the prediction for both individual predictions (local) and the entire model (global). This is crucial for ensuring that the fraud detection system is explainable, especially in sensitive fields like healthcare. The **Beeswarm Plot** and **Waterfall Plot** are two key visualizations that allow us to see these impacts clearly.\n",
    "\n",
    "### **SHAP Visualizations**:\n",
    "1. **Beeswarm Plot (Global Feature Importance)**  \n",
    "   The **beeswarm plot** shows the impact of each feature on the model's predictions for all test data. Features such as **Feature 3** and **Feature 0** emerged as the most influential, indicating their importance in predicting fraud cases.\n",
    "\n",
    "   ![SHAP Beeswarm Plot](ML_Images/shap_beeswarm_plot.png)\n",
    "\n",
    "2. **Waterfall Plot (Local Explanation for a Single Prediction)**  \n",
    "   The **waterfall plot** explains why the model flagged a particular provider as fraudulent. For this provider, **Feature 3** significantly contributed to increasing the fraud score, while **Feature 19** had a lower influence, reducing the overall score.\n",
    "\n",
    "   ![SHAP Waterfall Plot](ML_Images/shap_waterfall_plot.png)\n",
    "\n",
    "### **Key Stats from the SHAP Plots**:\n",
    "\n",
    "- **Global Insights**:  \n",
    "  From the **Beeswarm Plot**, we observed that:\n",
    "  - **Feature 3** has a strong positive impact on predicting fraud, meaning providers with higher values for **Feature 3** are more likely to be flagged as fraudulent.\n",
    "  - **Feature 19** was consistently associated with a negative impact, suggesting it reduces the likelihood of fraud being detected.\n",
    "\n",
    "- **Local Explanation**:  \n",
    "  The **Waterfall Plot** for a specific provider revealed:\n",
    "  - **Feature 3** pushed the fraud score higher by +2.89, while **Feature 19** reduced it by -0.64.\n",
    "  - The final model output (`f(x) = -0.729`) indicates the provider's prediction for fraud (with negative values corresponding to non-fraudulent).\n",
    "\n",
    "### **Next Steps**:\n",
    "By using SHAP, we not only gained **model transparency** but also improved the **trustworthiness** of our fraud detection system. This enables auditors and healthcare providers to confidently understand and challenge model decisions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90436c1-1e5e-4c44-ad4f-ab39f6b208aa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The **Healthcare Insurance Fraud Detection** project provides a comprehensive analysis of patterns in healthcare claims data to predict and understand fraudulent behavior. By leveraging multiple datasets (claims, inpatient, outpatient, and beneficiary details) and applying a combination of exploratory data analysis, machine learning, and explainable AI (SHAP), we derived several important insights:\n",
    "\n",
    "### 1. **Fraud Risk Factors Identified**\n",
    "   - **High reimbursement amounts**, **length of stay**, and the **presence of chronic conditions** were strong indicators of fraudulent behavior, highlighting key features that contribute to fraud risk.\n",
    "   - Providers with **higher claim volumes** were more likely to exhibit fraudulent behavior, supporting the hypothesis that higher billing rates correlate with an increased risk of fraud.\n",
    "\n",
    "### 2. **Machine Learning Model Performance**\n",
    "   - **XGBoost** emerged as the best-performing model, providing a good balance between **precision** and **recall** for fraud detection. It outperformed **Random Forest** and **Logistic Regression** in terms of detecting fraudulent cases while maintaining a high degree of accuracy.\n",
    "   - **Logistic Regression** performed well in identifying fraudulent cases but struggled with precision, showing that **class imbalance** was an issue that required fine-tuning of thresholds and weighting.\n",
    "\n",
    "### 3. **Explainability with SHAP**\n",
    "   - We implemented **SHAP (SHapley Additive exPlanations)** to make the machine learning models more **transparent** and **trustworthy**. SHAP visualizations helped us understand which features were most influential in predicting fraud, thus providing insights into model decisions.\n",
    "   - Both **global** and **local** SHAP plots showed that features like **reimbursement amounts** and **chronic conditions** were pivotal in classifying a provider as fraudulent or non-fraudulent.\n",
    "\n",
    "### 4. **Fraud Risk Scoring**\n",
    "   - By using the trained models to calculate a **fraud risk score** for each provider, we can prioritize investigations and direct resources where they are most needed. This risk score can help insurance companies and auditors focus on high-risk providers, potentially saving time and reducing costs.\n",
    "\n",
    "### **Broader Implications**\n",
    "\n",
    "The insights gained from this analysis have profound implications for improving healthcare fraud detection:\n",
    "- **Proactive Fraud Detection**: The models can be deployed in real-time to identify high-risk providers and claims, enabling early intervention.\n",
    "- **Cost Reduction**: By using AI to flag fraudulent claims, insurance companies can reduce the financial burden of fraud and improve their operational efficiency.\n",
    "- **Trust and Transparency**: The use of **explainable AI** helps foster trust in the system, as stakeholders can clearly understand how fraud predictions are made.\n",
    "\n",
    "### **Impact on the Community**\n",
    "\n",
    "The impact of this project extends beyond just improving fraud detection within the insurance industry. By reducing healthcare fraud, we directly contribute to:\n",
    "- **Lower healthcare costs**: Reducing fraud prevents unnecessary charges, leading to more affordable healthcare for individuals and communities.\n",
    "- **Improved resource allocation**: By flagging fraudulent providers, resources can be better allocated to those who need them, ensuring that genuine cases receive timely attention.\n",
    "- **Increased trust in the healthcare system**: The transparency and explainability of AI models help build trust among both providers and patients, fostering a more cooperative healthcare environment.\n",
    "- **Support for regulatory bodies**: This system can also support healthcare regulators in enforcing policies more effectively, ensuring that fraud is detected before it becomes widespread.\n",
    "\n",
    "This project demonstrates how AI and machine learning can not only streamline fraud detection but also play a critical role in creating a more equitable and efficient healthcare system. The ability to detect and mitigate fraud contributes to long-term sustainability, ensuring that resources are allocated fairly and transparently.\n",
    "\n",
    "### **Limitations and Future Work**\n",
    "\n",
    "While the project has provided valuable insights, it has certain limitations:\n",
    "- **Class Imbalance**: Despite adjustments, fraud remains a minority class, which poses challenges in achieving a perfectly balanced prediction.\n",
    "- **Data Gaps**: We were unable to incorporate external data (e.g., audit findings or external fraud reports) that could provide a more comprehensive view of fraud detection.\n",
    "- **Model Generalization**: The model may need adjustments when deployed in a real-world setting with constantly changing fraud strategies.\n",
    "\n",
    "Future research could:\n",
    "- Incorporate **additional datasets**, such as audit data or claims review outcomes.\n",
    "- Experiment with more advanced **ensemble models** or **deep learning techniques** to improve model accuracy.\n",
    "- **Expand the scope** by incorporating natural language processing (NLP) to process unstructured data from provider notes or appeals.\n",
    "\n",
    "By combining **AI-driven fraud detection** with **explainable AI**, this project lays the foundation for more effective, transparent, and scalable healthcare fraud prevention systems, with a direct positive impact on both the healthcare industry and the communities it serves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13ab4d-4390-4492-89f5-146364f0696a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
